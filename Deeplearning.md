# 1/30 DeepLearning


## 비지도학습 vs 지도학습

1. "비지도학습" vs "지도학습"

    - 지도학습
      - AI에이전트가 레이블에 접근 가능
      - 이를통해, 일부작업에서 성능 향상 가능

    - 비지도학습
      - **레이블(정답데이터)사용 불가**
      - 모델 성능의 정확한 측정은 불가능(측정기준x)
      - 하지만 중요한 feature를 발견해내는 것은 가능하다.

2. 장/단점
    - 지도학습
        - 장점:
          - 레이블이 충분히 확보되고 명확히 정의된 자료에서 확실한 모델 성능 향상가능
          - 여러 조건(컴퓨팅파워, 적절한 model 양질의 충분한 데이터셋)이 충족되면 우수한 model 만들 수 있음
          -  cost function 통해 성능을 측정 가능
        - 단점: 
          - 레이블 지정에 많은 비용 듦
          - 사전 학습한 레이블이 지정된 항목 이상 지식을 밝히거나 일반화 하는것은 한계 존재
          - 현실 세계 데이터는 대부분 레이블이 존재x
    -  비지도학습
       -  장점: 
          -  레이블이 충분치 않거나 패턴이 정형화 되지 않거나 데이터가 충분치 못한등 제약이이 있는 경우에도 학습 가능
          -  레이블이 아닌 데이터의 내재된 구조를 학습하여 작동 
          - 데이터 안의 숨겨진 패턴을 찾아내는데 유용하고
          - 개방적 문제의 해결 및 지식을 일반화 하는데 적합하다.
       - 단점:
         - 명확하게 정의되고 레이블이 충분한 데이터의 경우 지도학습에 비해 성능이 떨어질 수 있다.    

## 비지도 학습을 통한 문제 해결 

1. 과대적합
   - 비지도학습은 머신러닝이 지나치게 복잡하게 학습된 경우 이를 해소해줄 수 있는 '정규화기'로서의 역할을 수행할 수 있다.

   - 기존의 데이터 노이즈를 제거하고 본질적인 구조를 포착할 수 있게 도와준다.

2. 차원의 저주
   - 차원이 증가할수록 데이터의 분포 또는 모델추정에 필요한 샘플데이터의 개수가 많이 증가하게 되는데 이런 어려움을 표현한 것이 차원의 저주이다.

   - 비지도학습은 차원 감소를 통해 원래 피처에서 핵심적인 피처를 찾고 여기서 중요한 정보를 보존함으로써 차원의수를 최소한으로 줄인 뒤에, 지도학습을 통해 효율적인 근사치 함수를 구할 수 있다.
3. 피처 엔지니어링
   - 모델 성능을 향상 시키기 위해서는 좋은 feature를 생성하거나 구분할 수 있어야한다.      
   - 비지도학습 알고리즘을 통해 적절한 유형의 피처 표현을 자동으로 학습하게 하여 추출할 수 있다.
  
4. 이상치 제거 
   - 클러스터링등 비지도학습이 방법을 통해 이상치를 탐지하여 제거할 수 있다.


## 차원축소
- 정의 : 고차원의 데이터로부터 저차원의 데이터로 변환하는 방법

- 방법: feature selection과 feature extraction의 2가지 방법 존재함.

- 목적 
  -  더 나은 성능의 모델을 만들기 위해
  - 이해하기 쉬운 모델을 만들기 위해
  - 더 빠르게 실행되는 모델을 만들기 위해

- feature selection: 
  
  기존의 변수들중 상관관계가 존재하는 변수를 추출한다
  따라서 , 기존 데이터의 성질을 유지하며 데이터를 줄인다는 장점이 있지만 feature을 압축하지 않는다는 단점 또한 있다.


- feature extraction:

    기존의 변수들을 변환해 새로운 변수를 추출한다
    각각의 변수가 내재한 정보가 많아지지만 사람이 이를 해석할 수 없다는 단점도 있다.

[참고하면 좋을 포스팅](https://aytekin.tistory.com/49)


## Manifold learning

- 정의: Manifold Learning이란 고차원 데이터가 있을 때 **고차원 데이터를 데이터 공간에 뿌리면 샘플들을 잘 아우르는 subspace가 있을 것이라 가정에서 학습을 진행하는 방법**이다. 
  
  Manifold Learning은 차원축소를 위해 사용하며 이를 통해 고차원 데이터를 저차원에서도 잘 표현하는 공간인 manifold를 찾아 차원을 축소시킨다.

![차원의 저주](https://blog.kakaocdn.net/dn/EkeaH/btrewxDoAEp/SWTLQILuSUASbSL4nICQ6K/img.png)
고차원으로 이동할수록 데이터의 분포가 희박해지는데(밀도가 낮아지는데) 이 경우, 데이터의 특성을 제대로 반영하기가 어려워진다. 이를 해결하기 위해서 manifold learning이 필요하다.

![차원에 따른 거리 차이](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbpn3EN%2FbtrM37MAEyf%2FbtksIMW1q76jNK6viEDEAK%2Fimg.png)
또한 다음 그림과 같이, 고차원에서 보이는 데이터간의 거리와 실제 이를 저차원 공간으로 옮겨놓았을 때, 데이터의 거리는 큰 차이가 있을수 있다. 또 의미적으로도 완전히 다른 의미를 가질 수 있다.

- 방법 및 분류

1. 선형: PCA, LDA가 있음.

2. 비선형: t-sne,Kernel PCA,LLE,ISOMAP,MDS,AE

![manifold 방법/분류](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbNlSwO%2FbtrM7kjMUnx%2F3a2M7spM6VVWqUYgR0Dm91%2Fimg.png)


[참조하면 좋을 포스팅](https://roytravel.tistory.com/105)